---
title: "Data"
author: 
date: 
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---
This document is meant to be the hub for acquiring the data for the DAP II final project.


### Census API pull
This step uses the .acs5.get() function to pull the relevant tables from the US Census API/

```{python}
import os
import pandas as pd
import requests
from census import Census
from us import states

# Initialize the Census API with your API key
#Ella's Census Key: 4baba8df9d607f462b00b40dea2f2e359b2b8fe6
census_key = "28685c677403ed1719764eb8ba7a6087d00544fb"
c = Census(census_key)

# Define the output directory and file name
directory = r"C:\Users\laine\OneDrive\Documents\GitHub\DAPII_final"
file_name = os.path.join(directory, "B25075_raw.csv")


# Check if the CSV file already exists
if os.path.exists(file_name):
    print("Data file already exists. Skipping API call.")
else:
    variables = [f"B25075_{str(i).zfill(3)}E" for i in range(1, 26)]
    county_code = "031"  # Cook County
    state_code = "17"  # Illinois
    years = range(2010, 2023)
    all_data = []

    for year in years:
        data = c.acs5.state_county_tract(
            fields=["NAME"] + variables,
            state_fips=state_code,
            county_fips=county_code,
            tract="*",
            year=year
        )
        
        df = pd.DataFrame(data)
        df['year'] = year
        all_data.append(df)

    # Combine all data into a single DataFrame
    combined_data = pd.concat(all_data, ignore_index=True)
    combined_data.to_csv(file_name, index=False)
```

```{python}
# Cleaing the combined_data DF

# Define the metadata (labels from Census) URL for the table
metadata_url = "https://api.census.gov/data/2020/acs/acs5/variables"

# Fetch metadata for relevant variables
response = requests.get(metadata_url)
metadata = response.json()

# The rest of this relies on converting the metadata into a dictionary (I'm open to other suggestions how to do this? Seems maybe too complicated?)

# Extract headers and data
headers = metadata[0]  # ['name', 'label', 'concept']
data = metadata[1:]    # Remaining rows

# Filter metadata for variables belonging to table B25075
table_prefix = "B25075_"
filtered_data = [row for row in data if row[0].startswith(table_prefix)]

# Create a dictionary with 'name' as the key and 'label' as the value
metadata_dict = {row[0]: row[1] for row in filtered_data}

# Renaming
explore.rename(columns=metadata_dict, inplace=True)
explore.columns = explore.columns.str.replace("!!", " ", regex=False)
```


### City of Chicago Vacant Lot API Pull
 
This section grabs data from [this](https://data.cityofchicago.org/resource/kc9i-wq85.csv) CoC source.

```{python}
# make sure to install this package before running:
# pip install sodapy
from sodapy import Socrata

client = Socrata("data.cityofchicago.org", None)

# Hard coded total rows (5004), as results are limited at 1000 per query.
# Returned as JSON from API / converted to Python list of dictionaries by sodapy.
results = client.get("kc9i-wq85", limit=5004)

# Convert to pandas DataFrame
vacant_df = pd.DataFrame.from_records(results)
vacant_df.to_csv('vacant.csv') 
```


### Checkpoint; data has been acquired
## Set Directory

Change as needed while collaborating and define all file paths as ditectory + '_'
```{python}
directory = r"C:\Users\EM\Documents\GitHub\DAPII_final"
```


```{python}
census_data = pd.read_csv(directory + "\B25075_raw.csv")

vacancy_data = pd.read_csv(directory + "\\vacant.csv")

```


```{python}
import geopandas as gpd

geo_data=gpd.read_file(directory + "\geo_export_babed850-df6f-42f1-a951-f174092d5ba8.shp")

```

```{python}
geo_data.head()
```

```{python}
census_data.head()
```

```{python}
#allign names for census_data and geo_data
census_data['NAME']=census_data['NAME'].str.replace(", Cook County, Illinois", "")

```

```{python}
merged_census_and_geo=census_data.merge(geo_data, left_on='NAME', right_on='namelsad10', how='left')
merged_census_and_geo=gpd.GeoDataFrame(merged_census_and_geo, geometry='geometry')
merged_census_and_geo.head()
```

### Plotting Data
Here we will test our geo data

```{python}
import altair as alt
import matplotlib.pyplot as plt
from shapely.geometry import Point


#create a geometry column from the coordinates
vacancy_data['geometry'] = vacancy_data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

#transform vacancy location column into readable geographic data
vacancy_gdf = gpd.GeoDataFrame(vacancy_data, geometry='geometry')

#set coordinate ref system
vacancy_gdf.set_crs(epsg=4326, inplace=True) 

#plot tracts and points; used gen AI for simple plot

fig, ax = plt.subplots(figsize=(10, 10))

# Plot the city tracts
geo_data.plot(ax=ax, color='white', edgecolor='black')

# Plot the points
vacancy_gdf.plot(ax=ax, color='red', markersize=5)

plt.show()

```