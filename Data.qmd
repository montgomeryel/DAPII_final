---
title: "Data"
author: 
date: 
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---
This document is meant to be the hub for acquiring the data for the DAP II final project.


### Census API pull
This step uses the .acs5.get() function to pull the relevant tables from the US Census API/

```{python}
import os
import pandas as pd
import requests
from census import Census
from us import states
```
```{python}

# Initialize the Census API with your API key
#Ella's Census Key: 4baba8df9d607f462b00b40dea2f2e359b2b8fe6
census_key = "28685c677403ed1719764eb8ba7a6087d00544fb"
c = Census(census_key)

# Define the output directory and file name
directory = r"C:\Users\laine\OneDrive\Documents\GitHub\DAPII_final"
file_name = os.path.join(directory, "B25075_raw.csv")


# Check if the CSV file already exists
if os.path.exists(file_name):
    print("Data file already exists. Skipping API call.")
else:
    variables = [f"B25075_{str(i).zfill(3)}E" for i in range(1, 26)]
    county_code = "031"  # Cook County
    state_code = "17"  # Illinois
    years = range(2010, 2023)
    all_data = []

    for year in years:
        data = c.acs5.state_county_tract(
            fields=["NAME"] + variables,
            state_fips=state_code,
            county_fips=county_code,
            tract="*",
            year=year
        )
        
        df = pd.DataFrame(data)
        df['year'] = year
        all_data.append(df)

    # Combine all data into a single DataFrame
    combined_data = pd.concat(all_data, ignore_index=True)
    combined_data.to_csv(file_name, index=False)
```

```{python}
# Cleaing the combined_data DF

# Define the metadata (labels from Census) URL for the table
metadata_url = "https://api.census.gov/data/2020/acs/acs5/variables"

# Fetch metadata for relevant variables
response = requests.get(metadata_url)
metadata = response.json()

# The rest of this relies on converting the metadata into a dictionary (I'm open to other suggestions how to do this? Seems maybe too complicated?)

# Extract headers and data
headers = metadata[0]  # ['name', 'label', 'concept']
data = metadata[1:]    # Remaining rows

# Filter metadata for variables belonging to table B25075
table_prefix = "B25075_"
filtered_data = [row for row in data if row[0].startswith(table_prefix)]

# Create a dictionary with 'name' as the key and 'label' as the value
metadata_dict = {row[0]: row[1] for row in filtered_data}

# Renaming
explore.rename(columns=metadata_dict, inplace=True)
explore.columns = explore.columns.str.replace("!!", " ", regex=False)
```


### City of Chicago Vacant Lot API Pull
 
This section grabs data from [this](https://data.cityofchicago.org/resource/kc9i-wq85.csv) CoC source.

```{python}
# make sure to install this package before running:
# pip install sodapy
from sodapy import Socrata

client = Socrata("data.cityofchicago.org", None)

# Hard coded total rows (5004), as results are limited at 1000 per query.
# Returned as JSON from API / converted to Python list of dictionaries by sodapy.
results = client.get("kc9i-wq85", limit=5004)

# Convert to pandas DataFrame
vacant_df = pd.DataFrame.from_records(results)
vacant_df.to_csv('vacant.csv') 
```


### Checkpoint; data has been acquired
## Set Directory

Change as needed while collaborating and define all file paths as ditectory + '_'
```{python}
directory = r"C:\Users\laine\OneDrive\Documents\GitHub\DAPII_final"
```


```{python}
census_data = pd.read_csv(directory + r"\B2575_raw.csv")

vacancy_data = pd.read_csv(directory + r"\vacant.csv")

```

```{python}
census_data.rename(columns=metadata_dict, inplace=True)
census_data.columns = census_data.columns.str.replace("!!", " ", regex=False)
```

```{python}
census_data.head()
```
```{python}
import geopandas as gpd

geo_data=gpd.read_file(directory + r"\geo_export_babed850-df6f-42f1-a951-f174092d5ba8.shp")

```

```{python}
geo_data.head()
```


```{python}
#allign names for census_data and geo_data
census_data['NAME']=census_data['NAME'].str.replace(", Cook County, Illinois", "")

```

```{python}
merged_census_and_geo=census_data.merge(geo_data, left_on='NAME', right_on='namelsad10', how='left')
merged_census_and_geo=gpd.GeoDataFrame(merged_census_and_geo, geometry='geometry')
merged_census_and_geo.head()
```

### Plotting Data
Here we will test our geo data

```{python}
import altair as alt
import matplotlib.pyplot as plt
from shapely.geometry import Point


#create a geometry column from the coordinates
vacancy_data['geometry'] = vacancy_data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

#transform vacancy location column into readable geographic data
vacancy_gdf = gpd.GeoDataFrame(vacancy_data, geometry='geometry')

#set coordinate ref system
vacancy_gdf.set_crs(epsg=4326, inplace=True) 

#plot tracts and points; used gen AI for simple plot

fig, ax = plt.subplots(figsize=(10, 10))

# Plot the city tracts
geo_data.plot(ax=ax, color='white', edgecolor='black')

# Plot the points
vacancy_gdf.plot(ax=ax, color='red', markersize=5)

plt.show()

```

```{python}
merged_census_and_geo.columns[2:26]
```

### Housing Price Choropleth 
```{python}
 def ave_price(row):
    # Multiplying values from columns based on the row
    calc_1 = row['Estimate Total: Less than $10,000'] * 10000
    calc_2 = row['Estimate Total: $10,000 to $14,999'] * 14999
    calc_3 = row[merged_census_and_geo.columns[4]] * 19999
    calc_4 = row[merged_census_and_geo.columns[5]] * 19999
    calc_5 = row[merged_census_and_geo.columns[6]] * 24999
    calc_6 = row[merged_census_and_geo.columns[7]] * 29999
    calc_7 = row[merged_census_and_geo.columns[8]] * 34999
    calc_8 = row[merged_census_and_geo.columns[9]] * 39999
    calc_9 = row[merged_census_and_geo.columns[10]] * 39999
    calc_10 = row[merged_census_and_geo.columns[11]] * 49999
    calc_11 = row[merged_census_and_geo.columns[12]] * 69999
    calc_12 = row[merged_census_and_geo.columns[13]] * 79999
    calc_13 = row[merged_census_and_geo.columns[14]] * 89999
    calc_14 = row[merged_census_and_geo.columns[15]] * 99999
    calc_15 = row[merged_census_and_geo.columns[16]] * 124999
    calc_16 = row[merged_census_and_geo.columns[17]] * 149999
    calc_17 = row[merged_census_and_geo.columns[18]] * 174999
    calc_18 = row[merged_census_and_geo.columns[19]] * 199999
    calc_19 = row[merged_census_and_geo.columns[20]] * 249999
    calc_20 = row[merged_census_and_geo.columns[21]] * 299999
    calc_21 = row[merged_census_and_geo.columns[22]] * 399999
    calc_22 = row[merged_census_and_geo.columns[23]] * 499999
    calc_23 = row[merged_census_and_geo.columns[24]] * 749999
    calc_24 = row[merged_census_and_geo.columns[25]] * 999999
    calc_25 = row[merged_census_and_geo.columns[26]] * 1499999

    numerator = (calc_1 + calc_2 + calc_3 + calc_4 + calc_5 + calc_6 +
                 calc_7 + calc_8 + calc_9 + calc_10 + calc_11 + calc_12 +
                 calc_13 + calc_14 + calc_15 + calc_16 + calc_17 + calc_18 +
                 calc_19 + calc_20 + calc_21 + calc_22 + calc_23 + calc_24 +
                 calc_25)

    if row['Estimate Total:']==0:
      average=0
    else:
      denominator = row['Estimate Total:']
      average = numerator / denominator
    return average


# Apply the function to each row of the DataFrame
merged_census_and_geo['average_price'] = merged_census_and_geo.apply(ave_price, axis=1)

 ```
 
Create plot
```{python}
fig, ax = plt.subplots(figsize=(10, 10))

# Plot the city tracts
merged_census_and_geo.plot(ax=ax, column='average_price', edgecolor='black').set_axis_off()
ax.legend(loc='upper right')

#Plot the points
vacancy_gdf.plot(ax=ax, color='red', markersize=5)

plt.show()
```